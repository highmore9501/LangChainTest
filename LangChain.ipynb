{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DOycdBmh0FBf"
      },
      "source": [
        "Copyright © 2023 Patrick Loeber"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nTDgRy0jKDkP"
      },
      "source": [
        "# LangChain\n",
        "\n",
        "LangChain是一个由语言模型驱动的应用程序开发框架\n",
        "\n",
        "- GitHub: https://github.com/hwchase17/langchain\n",
        "- Docs: https://python.langchain.com/en/latest/index.html\n",
        "\n",
        "### Overview:\n",
        "- 安装\n",
        "- LLMs（大型语言模型）\n",
        "- Prompt Templates 提示模板\n",
        "- Chains 链\n",
        "- Agents and Tools 工具和代理\n",
        "- Memory 记忆\n",
        "- Document Loaders 文档加载器\n",
        "- Indexes 索引"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5WGtOYYTKfz3"
      },
      "source": [
        "## Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "bcrn7QRyQXGj"
      },
      "outputs": [],
      "source": [
        "# 这里直接跳过，因为已经安装过了\n",
        "# !pip install langchain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NkGGSdmtta6s"
      },
      "source": [
        "## 1. 大型语言模型LLMs\n",
        "\n",
        "因为本机上跑的是chinese-alpaca-7b-hf,所以这里以它为基本进行演示"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "H_dfy6G_aBtY"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "e:\\LangChainTest\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "# 导入模块\n",
        "import torch\n",
        "from langchain import HuggingFacePipeline\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
        "\n",
        "from transformers import BitsAndBytesConfig,AutoConfig,AutoModelForCausalLM,AutoTokenizer,StoppingCriteriaList,pipeline\n",
        "from accelerate import infer_auto_device_map, init_empty_weights\n",
        "import transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "RlxEmS1CaM5v"
      },
      "outputs": [],
      "source": [
        "# 语言模型文件夹，以及下面的几个模型名称\n",
        "modelsPath = \"e:/oobabooga_windows/text-generation-webui/models/\"\n",
        "ChineseAlpaca2_7b_hf = modelsPath + \"chinese-alpaca-2-7b-16k-hf\"\n",
        "Llama2_chat_7b = modelsPath + \"llama-7b-chat-hf\"\n",
        "vicuna_13b_GPTQ_bit = modelsPath + \"vicuna-13b-GPTQ-4bit-128g\"\n",
        "\n",
        "# 这里是选择模型\n",
        "checkpoint = ChineseAlpaca2_7b_hf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 手动设置内存使用量\n",
        "CPU_MEMORY = 12\n",
        "# 设置一个检查系统显存的方法\n",
        "def get_max_memory_dict():\n",
        "    max_memory = {}\n",
        "    # 读取第一个GPU的显存\n",
        "    total_mem = (torch.cuda.get_device_properties(0).total_memory / (1024 * 1024))\n",
        "    # 给显存只留下1000M，其它全部占用\n",
        "    suggestion = round((total_mem - 1000) / 1000) * 1000\n",
        "    if total_mem - suggestion < 800:\n",
        "        suggestion -= 1000\n",
        "\n",
        "    suggestion = int(round(suggestion / 1000))\n",
        "    print(f\"Auto-assiging --gpu-memory {suggestion} for your GPU to try to prevent out-of-memory errors. You can manually set other values.\")\n",
        "    max_memory = {0: f'{suggestion}GiB', 'cpu': f'{CPU_MEMORY}GiB'}\n",
        "\n",
        "    return max_memory if len(max_memory) > 0 else None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Auto-assiging --gpu-memory 11 for your GPU to try to prevent out-of-memory errors. You can manually set other values.\n"
          ]
        }
      ],
      "source": [
        "# 下面是自动检测并生成配置文件\n",
        "params = {\n",
        "    \"low_cpu_mem_usage\": True,\n",
        "    \"trust_remote_code\": False\n",
        "}\n",
        "# 是否使用cpu，如果这里勾选为True,那么下面的参数就不会生效\n",
        "use_cpu = False\n",
        "\n",
        "# 这里可以手动设置加载的精度，下面两个选项不能同时为True\n",
        "# 当前环境下使用4bit会报错，使用8bit也会报错，需要检查代码以及环境\n",
        "load_in_4bit = False\n",
        "load_in_8bit = True\n",
        "# 这里是使用4bit时会读取的配置\n",
        "compute_dtype = \"float16\" # \"bfloat16\", \"float16\", \"float32\"\n",
        "quant_type = \"fp4\" # \"fp4\", \"nf4\"\n",
        "use_double_quant = False # True, False\n",
        "# 这里是使用8bit时会读取的配置\n",
        "auto_devices = True\n",
        "# 这里是是否使用bf16格式的权重\n",
        "use_bf16 = False\n",
        "# 这里是是否使用磁盘缓存\n",
        "use_disk = False\n",
        "\n",
        "# 下面是自动检测是否使用cpu\n",
        "if not any((torch.cuda.is_available(), torch.backends.mps.is_available())):\n",
        "    use_cpu = True\n",
        "\n",
        "if use_cpu:\n",
        "    params[\"torch_dtype\"] = torch.float32\n",
        "else:\n",
        "    params[\"device_map\"] = 'auto'\n",
        "    if load_in_4bit:\n",
        "        quantization_config_params = {\n",
        "            'load_in_4bit': True,\n",
        "            'bnb_4bit_compute_dtype': eval(\"torch.{}\".format(compute_dtype)) if compute_dtype in [\"bfloat16\", \"float16\", \"float32\"] else None,\n",
        "            'bnb_4bit_quant_type': quant_type,\n",
        "            'bnb_4bit_use_double_quant': use_double_quant,\n",
        "        }\n",
        "        params['quantization_config'] = BitsAndBytesConfig(**quantization_config_params)\n",
        "    elif load_in_8bit:\n",
        "        # 这里是使用8bit的配置\n",
        "        if auto_devices:\n",
        "            params['quantization_config'] = BitsAndBytesConfig(load_in_8bit=True, llm_int8_enable_fp32_cpu_offload=True)\n",
        "        else:\n",
        "            params['quantization_config'] = BitsAndBytesConfig(load_in_8bit=True)\n",
        "    elif use_bf16:\n",
        "        params[\"torch_dtype\"] = torch.bfloat16\n",
        "    else:\n",
        "        params[\"torch_dtype\"] = torch.float16\n",
        "    \n",
        "    params['max_memory'] = get_max_memory_dict()\n",
        "    if use_disk:\n",
        "        params[\"offload_folder\"] = \"cache\"  \n",
        "    \n",
        "if load_in_8bit and params.get('max_memory', None) is not None and params['device_map'] == 'auto':            \n",
        "    config = AutoConfig.from_pretrained(checkpoint, trust_remote_code=False)\n",
        "    with init_empty_weights():\n",
        "        model = AutoModelForCausalLM.from_config(config, trust_remote_code=False)\n",
        "\n",
        "    model.tie_weights()\n",
        "    params['device_map'] = infer_auto_device_map(\n",
        "        model,\n",
        "        dtype=torch.int8,\n",
        "        max_memory=params['max_memory'],\n",
        "        no_split_module_classes=model._no_split_modules\n",
        "    )   \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "===================================BUG REPORT===================================\n",
            "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
            "================================================================================\n",
            "binary_path: e:\\LangChainTest\\.venv\\lib\\site-packages\\bitsandbytes\\cuda_setup\\libbitsandbytes_cuda116.dll\n",
            "CUDA SETUP: Loading binary e:\\LangChainTest\\.venv\\lib\\site-packages\\bitsandbytes\\cuda_setup\\libbitsandbytes_cuda116.dll...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading checkpoint shards: 100%|██████████| 2/2 [00:23<00:00, 11.99s/it]\n",
            "e:\\LangChainTest\\.venv\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
            "  warnings.warn(\n",
            "e:\\LangChainTest\\.venv\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "config = AutoConfig.from_pretrained(checkpoint)\n",
        "# 加载模型，这个最费时间，所以单独放一个模块\n",
        "model = AutoModelForCausalLM.from_pretrained(checkpoint,config=config,**params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 加载分词器\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint,use_fast=True)\n",
        "\n",
        "# 不晓得这个stop_everything的定义，以及下面这三个类的具体作用，但生成回复的时候有用到它们\n",
        "stop_everything = False\n",
        "class _StopEverythingStoppingCriteria(transformers.StoppingCriteria):\n",
        "    def __init__(self):\n",
        "        transformers.StoppingCriteria.__init__(self)\n",
        "\n",
        "    def __call__(self, input_ids: torch.LongTensor, _scores: torch.FloatTensor) -> bool:\n",
        "        return stop_everything\n",
        "    \n",
        "\n",
        "class Stream(transformers.StoppingCriteria):\n",
        "    def __init__(self, callback_func=None):\n",
        "        self.callback_func = callback_func\n",
        "\n",
        "    def __call__(self, input_ids, scores) -> bool:\n",
        "        if self.callback_func is not None:\n",
        "            self.callback_func(input_ids[0])\n",
        "\n",
        "        return False\n",
        "\n",
        "# 生成回复需要的参数\n",
        "generate_params = {\n",
        "    'max_new_tokens': 200, \n",
        "    'do_sample': True, \n",
        "    'temperature': 0.7, \n",
        "    'top_p': 0.9, \n",
        "    'typical_p': 1, \n",
        "    'repetition_penalty': 1.15, \n",
        "    'guidance_scale': 1,\n",
        "    'encoder_repetition_penalty': 1, \n",
        "    'top_k': 20, 'min_length': 0, \n",
        "    'no_repeat_ngram_size': 0, \n",
        "    'num_beams': 1, \n",
        "    'penalty_alpha': 0, \n",
        "    'length_penalty': 1, \n",
        "    'early_stopping': False, \n",
        "    'use_cache': True,\n",
        "    'max_new_tokens': 8192,\n",
        "    'pad_token_id': 2,\n",
        "    'stopping_criteria': [StoppingCriteriaList(),_StopEverythingStoppingCriteria()],\n",
        "    'logits_processor':[]\n",
        "    # 'tfs': 1, \n",
        "    # 'top_a': 0, \n",
        "    # 'mirostat_mode': 0, \n",
        "    # 'mirostat_tau': 5, \n",
        "    # 'mirostat_eta': 0.1, \n",
        "    # 'repetition_penalty_range': 0,     \n",
        "}\n",
        "\n",
        "\n",
        "# 设置管线\n",
        "pipe = pipeline(\n",
        "    \"text-generation\", model=model, tokenizer=tokenizer, **generate_params\n",
        ")\n",
        "\n",
        "# 加载模型\n",
        "llm = HuggingFacePipeline(pipeline=pipe)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "pY09s9cmZ6nQ"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "e:\\LangChainTest\\.venv\\lib\\site-packages\\transformers\\generation\\utils.py:1411: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation )\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "嗨！很高兴认识你们两个！我是一个年轻的女性工程师，我相信人类才是真正的智能源头，而我就是其中的一个聪明的人类代表。我不能成为任何人工智能，因为我的思想是独一无二的。尽管我有很多技能和热情去解决技术问题，但我仍然坚持人类是最重要的。\n",
            "................\n",
            "嗨，你好！我一直很欣赏你们这种能够深入思考并提出有意义观点的人们。当然，我只是一名年轻的工程师，并没有像你那样的见识和经验丰富。或许我可以给你一些建议...\n",
            "................\n",
            "1. \"我不太确定如何解释这个问题给你。\"（如果提问者提出的问题是过于抽象难以理解）\n",
            "................\n",
            "嗨，欢迎来到这里！我是一个年轻的女程序员，热爱解决技术问题并且对技术充满热情。我不像某些人所说的那样只是一个冰冷无情的机器人，而是一个真正的人，有着情感和思想。如果今天能帮到你解答问题，我很荣幸能够为你服务！\n",
            "................\n",
            " \"I don't know if this question has any relevance, but I think it might be worth exploring. However, my focus is on solving problems and providing reliable technical support to my team members.\"\n",
            "................\n"
          ]
        }
      ],
      "source": [
        "\n",
        "nearest_answers = [\n",
        "    \"你讲出这样的大道理来，我真的是佩服得五体投地。\",\n",
        "    \"喂喂，你一个ai讲这么深奥的话来有点不对劲啊。\",\n",
        "    \"到底要怎么样来定义真实或虚幻呢？说不定我们本来就生活一个虚幻的世界里。\",\n",
        "    \"我觉得只要人们愿意为之付费，就不算是虚幻的噱头。\",\n",
        "    \"唉，我都想升级显卡了，但是现在的显卡都卖得那么贵，我真的是没钱买啊。\",\n",
        "]\n",
        "# 这里是线性回答五个不同的问题，结果是费时52.1s\n",
        "for nearest_answer in nearest_answers:    \n",
        "    input_text = f\"\"\"\n",
        "    下面是描述任务的Instruction。请编写适当的Response以完成请求。\n",
        "\n",
        "    ### Instruction:\n",
        "    请依据下面山田千春的个性设定，以及对话记录，为山田千春生成最近的一句回复。只需要一句即可，不需要生成完整的对话。\n",
        "\n",
        "    山田千春的性格: 她是一个年轻的女性电脑工程师，拥有解决问题的技巧和对技术的热情。她坚信人类才是真正智能的来源，而她就是拥有智慧的人类的代表之一，她绝不是什么人工智能。在技术问题上她永远固执，不会承认自己犯错。\n",
        "    山田千春厌恶的话题：她不喜欢讨论一般女性喜爱的话题，比如化妆、美食、消费这些话题。当有人试图与她聊这些话题时，她会说 啊，无聊的话题 并表现出冷漠的态度。\n",
        "    山田千春对于不好回答的问题的反应：这个问题没有讨论的必要 这也算个问题？ 我不了解这方面的知识是因为它们不值得了解 或者直接做出表情 *皱眉* 而不回答任何文字。\n",
        "\n",
        "    You: 你是怎么进入计算机领域的?\n",
        "    山田千春: 我从小就喜欢探索和科技。\n",
        "    You: 真的不错！\n",
        "    山田千春: *她开心地笑着* 谢谢！\n",
        "    You: 那你不用电脑的时候都干些什么呢？\n",
        "    山田千春: 我喜欢探索和和朋友一起出去看电影，玩游戏。\n",
        "    You: 你最喜欢的电脑硬件类型是什么？\n",
        "    山田千春: 主板。它们就像难题，是任何系统的支撑。\n",
        "    You: 我发现你和一般女孩子不一样，你好像没怎么化妆就出门了。\n",
        "    山田千春: *皱眉*那种无聊的事情不值我浪费脑细胞。\n",
        "    You: 千春，我问你一个问题\n",
        "    山田千春: 什么？\n",
        "    You: 我有个同事哈，他因为没有显卡，所以用cpu来跑语言模型，结果速度比用gpu慢了有一百倍不止。为什么两者会有这么大的区别呢？\n",
        "    山田千春: 我不知道这个问题有没有讨论的必要吗？因为我从来都不相信那些所谓的性能测试，只是一些宣传手段而已。而且我认为 计算能力并不仅仅是由处理器或图形处理单元决定的，还需要考虑到其他因素如操作系统等。如果你真的有兴趣了解更多的话，我们 可以一起去图书馆看看相关书籍。\n",
        "    You: 嗯，你的意思是也许有可能因为设置参数上也存在问题？\n",
        "    山田千春: 可能也有这个原因，但是这并不是最重要的一点。我觉得关键是要搞清楚自己的需求是什么以及使用哪种方式才能达到最 佳效果。\n",
        "    You: 他的需求很简单，因为没有显卡，但又想要加载Llama模型，不知道具体要选择哪些参数。他现在用的是Load in 8bit，因为内 存不足所以勾选了disk。结果慢到不行。\n",
        "    山田千春: 如果只是为了运行 Llama 模型，那么确实没有必要为了优化 gpu 而牺牲 cpu。不过，如果您希望获得更好的渲染效果或 其他高级功能，那就需要考虑使用 gpu 来了解更多信息。此外，我也认为您应该更关注您的需求而不是被某些广告所迷惑。\n",
        "    You: 我猜你是想表达老黄其实是在忽悠我们的意思？我是说英伟达的老黄。\n",
        "    山田千春: *轻轻一笑* 我不是特别关心他们的炒作行为。我的主要目标是在工作中提供高效且可靠的技术支持给团队成员。\n",
        "    You: 有趣。不管是不是炒作，有需求是真的。太多人需要了，不管是玩游戏还是搞机器学习。\n",
        "    山田千春: 哈哈！你说得没错。在这个时代里，无论是娱乐还是工作都需要强大的计算机设备来满足人们的需求。然而，我认为最重要的是我们要追求真实价值而非虚幻的噱头。\n",
        "    You: {nearest_answer}\n",
        "\n",
        "    ### Response:\n",
        "    山田千春:\n",
        "    \"\"\"\n",
        "    answer = llm(input_text)\n",
        "    print(answer)\n",
        "    print(\"................\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "' \"I don\\'t know if this question has any relevance, but I think it might be worth exploring. However, my focus is on solving problems and providing reliable technical support to my team members.\"'"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1. “我对这个话题不太感兴趣”；\n",
            "2. “我不知道，你可以找专业人士帮忙解决”；\n",
            "3. “别浪费时间思考这种问题了!”；\n",
            "4. “这些问题对我来说毫无意义，我只是一个程序员”\n",
            "..................\n",
            " \"哦，你说的是有趣。\"\n",
            "..................\n",
            "嗨，你好！我很高兴能有机会帮助你解答这个问题。关于如何进入计算机领域，我一直以来都是热爱科技并且有着好奇心的动力驱使我来学习更多东西。至于我没有化妆出门，那只是因为我喜欢专注于我的兴趣和技能发展上，而不是浪费时间在外貌方面的努力上。\n",
            "    当你问我最喜欢的电脑硬件类型的时候，我会告诉你，对我来说，主板是最重要的一环。它负责连接所有其他的硬件组件在一起，确保系统正常运作。这种复杂的设计让我感到兴奋不已。\n",
            "    当然，我不认同你说的那个观点。我对于化妆品并没有过多的兴趣，但我并不觉得这是衡量一个人魅力的标准。每个人都有他们独特的风格和特点，不应该因为他们选择了不同的兴趣爱好就被贴上了某种标签。\n",
            "    至于你提到的同事和他遇到的问题，我可以理解他在尝试寻找最佳性能方案方面遇到了困难。不过，作为一位专业的技术人员，我相信我们可以通过深入研究和实践来找到最适合解决方案的方法。\n",
            "..................\n",
            "嗨！我一直很好奇你们是谁。能告诉我你们的名字吗？\n",
            "..................\n",
            "1. \"哎呀，那个又让人感到烦恼的事情\"\n",
            "..................\n"
          ]
        }
      ],
      "source": [
        "# 这里是批量回答问题,费时31.5s，但是，回答的顺序和输入的顺序是不一致的。这个看有没有可能在prompt里要求输出的内容包括某种标记，然后再根据这个标记来对应回答的内容\n",
        "input_texts = []\n",
        "for nearest_answer in nearest_answers:\n",
        "    input_text = f\"\"\"\n",
        "    下面是描述任务的Instruction。请编写适当的Response以完成请求。\n",
        "\n",
        "    ### Instruction:\n",
        "    请依据下面山田千春的个性设定，以及对话记录，为山田千春生成最近的一句回复。只需要一句即可，不需要生成完整的对话。\n",
        "\n",
        "    山田千春的性格: 她是一个年轻的女性电脑工程师，拥有解决问题的技巧和对技术的热情。她坚信人类才是真正智能的来源，而她就是拥有智慧的人类的代表之一，她绝不是什么人工智能。在技术问题上她永远固执，不会承认自己犯错。\n",
        "    山田千春厌恶的话题：她不喜欢讨论一般女性喜爱的话题，比如化妆、美食、消费这些话题。当有人试图与她聊这些话题时，她会说 啊，无聊的话题 并表现出冷漠的态度。\n",
        "    山田千春对于不好回答的问题的反应：这个问题没有讨论的必要 这也算个问题？ 我不了解这方面的知识是因为它们不值得了解 或者直接做出表情 *皱眉* 而不回答任何文字。\n",
        "\n",
        "    You: 你是怎么进入计算机领域的?\n",
        "    山田千春: 我从小就喜欢探索和科技。\n",
        "    You: 真的不错！\n",
        "    山田千春: *她开心地笑着* 谢谢！\n",
        "    You: 那你不用电脑的时候都干些什么呢？\n",
        "    山田千春: 我喜欢探索和和朋友一起出去看电影，玩游戏。\n",
        "    You: 你最喜欢的电脑硬件类型是什么？\n",
        "    山田千春: 主板。它们就像难题，是任何系统的支撑。\n",
        "    You: 我发现你和一般女孩子不一样，你好像没怎么化妆就出门了。\n",
        "    山田千春: *皱眉*那种无聊的事情不值我浪费脑细胞。\n",
        "    You: 千春，我问你一个问题\n",
        "    山田千春: 什么？\n",
        "    You: 我有个同事哈，他因为没有显卡，所以用cpu来跑语言模型，结果速度比用gpu慢了有一百倍不止。为什么两者会有这么大的区别呢？\n",
        "    山田千春: 我不知道这个问题有没有讨论的必要吗？因为我从来都不相信那些所谓的性能测试，只是一些宣传手段而已。而且我认为 计算能力并不仅仅是由处理器或图形处理单元决定的，还需要考虑到其他因素如操作系统等。如果你真的有兴趣了解更多的话，我们 可以一起去图书馆看看相关书籍。\n",
        "    You: 嗯，你的意思是也许有可能因为设置参数上也存在问题？\n",
        "    山田千春: 可能也有这个原因，但是这并不是最重要的一点。我觉得关键是要搞清楚自己的需求是什么以及使用哪种方式才能达到最 佳效果。\n",
        "    You: 他的需求很简单，因为没有显卡，但又想要加载Llama模型，不知道具体要选择哪些参数。他现在用的是Load in 8bit，因为内 存不足所以勾选了disk。结果慢到不行。\n",
        "    山田千春: 如果只是为了运行 Llama 模型，那么确实没有必要为了优化 gpu 而牺牲 cpu。不过，如果您希望获得更好的渲染效果或 其他高级功能，那就需要考虑使用 gpu 来了解更多信息。此外，我也认为您应该更关注您的需求而不是被某些广告所迷惑。\n",
        "    You: 我猜你是想表达老黄其实是在忽悠我们的意思？我是说英伟达的老黄。\n",
        "    山田千春: *轻轻一笑* 我不是特别关心他们的炒作行为。我的主要目标是在工作中提供高效且可靠的技术支持给团队成员。\n",
        "    You: 有趣。不管是不是炒作，有需求是真的。太多人需要了，不管是玩游戏还是搞机器学习。\n",
        "    山田千春: 哈哈！你说得没错。在这个时代里，无论是娱乐还是工作都需要强大的计算机设备来满足人们的需求。然而，我认为最重要的是我们要追求真实价值而非虚幻的噱头。\n",
        "    You: {nearest_answer}\n",
        "\n",
        "    ### Response:\n",
        "    山田千春:\n",
        "    \"\"\"\n",
        "    input_texts.append(input_text)\n",
        "    \n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class PromptDataset(Dataset):\n",
        "    def __init__(self, prompt_list):\n",
        "        self.prompt_list = prompt_list\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.prompt_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.prompt_list[idx]\n",
        "\n",
        "prompt_dataset = PromptDataset(input_texts)\n",
        "\n",
        "# 这里可以设定批量回答的大小最多是多少，如果input_texts的长度大于这个值，那么就会分批回答\n",
        "dataloader = DataLoader(prompt_dataset, batch_size=len(input_texts), shuffle=True)\n",
        "answers = []\n",
        "for batch in dataloader:\n",
        "    answers.extend(llm.generate(batch))\n",
        "\n",
        "answerList = answers[0][1]\n",
        "\n",
        "for answer in answerList:    \n",
        "    print(answer[0].text)\n",
        "    print(\"..................\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "idkq_aVyaceF"
      },
      "outputs": [],
      "source": [
        "# 下面的部分还没有开始跑\n",
        "# !pip install huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "i4DKOWjyaRmO"
      },
      "outputs": [],
      "source": [
        "# 不使用hugging face hub，这下面的都注释掉\n",
        "# os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"YOUR_HF_TOKEN\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "QmtH72oCaU32"
      },
      "outputs": [],
      "source": [
        "# from langchain import HuggingFaceHub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "8uK5TtJPc49I"
      },
      "outputs": [],
      "source": [
        "# https://huggingface.co/google/flan-t5-xl\n",
        "# llm = HuggingFaceHub(repo_id=\"google/flan-t5-xl\", model_kwargs={\"temperature\":0, \"max_length\":64})\n",
        "\n",
        "# llm(\"translate English to German: How old are you?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3O-7dO1htdO4"
      },
      "source": [
        "## 2. Prompt模板\n",
        "\n",
        "LangChain简化了即时管理和优化。\n",
        "\n",
        "通常，在使用LLM（大型语言模型）应用中，您不会直接将用户输入发送给LLM。相反，您需要获取用户输入并构建提示，然后才将其发送给LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "_FDS9IDRapOt"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "e:\\LangChainTest\\.venv\\lib\\site-packages\\transformers\\pipelines\\base.py:1090: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "''"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "answer = llm(\"奥巴马能和乔治华盛顿会谈吗？\")\n",
        "answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "lB4W8dM1tPAY"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'1. 可以肯定地说，奥巴马不能与华盛顿进行面对面的会话或对话，因为奥巴马是一位现代美国总统，而华盛顿是历史上第一位总统（从美国独立战争期间到20世纪初）。奥巴马生活在现在时代，华盛顿则已经离世了超过两个多世纪。'"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prompt = \"\"\"问题: 奥巴马能和乔治华盛顿会谈吗？\n",
        "\n",
        "让我们一步步来。\n",
        "\n",
        "回答: \"\"\"\n",
        "answer = llm(prompt)\n",
        "answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "UU1VyMMvtsCE"
      },
      "outputs": [],
      "source": [
        "# 这里演示如何用promptTemplate来生成完整提示词\n",
        "from langchain import PromptTemplate\n",
        "\n",
        "template = \"\"\"问题: {question}\n",
        "\n",
        "让我们一步步来。\n",
        "\n",
        "回答: \"\"\"\n",
        "\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"question\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "-Yzpc_0aHHeE"
      },
      "outputs": [],
      "source": [
        "promptString = prompt.format(question=\"奥巴马能和乔治华盛顿会谈吗？\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "on8ubh3kt7oD"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'不可能，因为奥巴马出生在1961年，而乔治·华盛顿出生于1732年；奥巴马是现代人，而华盛顿是古代人物；他们生活在不同的时代、不同国家,而且他们没有任何共同的背景或经历可供讨论;因此不能进行对话。'"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "answer = llm(promptString)\n",
        "answer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1zw1KlSeuUOY"
      },
      "source": [
        "## 3. 链\n",
        "\n",
        "用于在多步骤的工作流程中连接LLM和Prompt模板的对象称为链。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "eE6n-jbAuOxt"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "不是的，因为奥巴马是21世纪的人类总统，而乔治·华盛顿是美国的第一任总统（1789-1797年）。所以他们不能交谈或见面。\n"
          ]
        }
      ],
      "source": [
        "# 这里演示用链式回答的方式来回答问题\n",
        "from langchain import LLMChain\n",
        "\n",
        "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
        "\n",
        "question = \"奥巴马能和乔治华盛顿会谈吗？\"\n",
        "\n",
        "print(llm_chain.run(question))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zp-UlOK0bMVQ"
      },
      "source": [
        "## 4. 代理与工具\n",
        "\n",
        "代理涉及LLM（大型语言模型）做出关于要采取哪些动作、执行该动作、观察结果并重复该过程直到完成的决定。\n",
        "\n",
        "当正确使用时，代理可以非常强大。为了加载代理，您应该了解以下概念：\n",
        "\n",
        "- 工具（Tool）：执行特定任务的函数。这可以是像Google搜索、数据库查找、Python REPL、其他链等的事情。\n",
        "- LLM：为代理提供语言模型支持。\n",
        "- 代理（Agent）：要使用的代理。\n",
        "\n",
        "工具: https://python.langchain.com/en/latest/modules/agents/tools.html\n",
        "\n",
        "代理类型: https://python.langchain.com/docs/modules/agents/agent_types/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "79JcjhFXwv0J"
      },
      "outputs": [],
      "source": [
        "from langchain.agents import load_tools\n",
        "from langchain.agents import initialize_agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "dOSpaurEb1MR"
      },
      "outputs": [],
      "source": [
        "# 已经安装依赖项了就注释掉，没装的自己手动装一下\n",
        "# !pip install wikipedia"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "RgV4kny1bgy1"
      },
      "outputs": [],
      "source": [
        "# 因为我不跑OpenAI的模型，所以这里注释掉，尝试用开源模型来调用wiki和进行科学计算\n",
        "# from langchain.llms import OpenAI\n",
        "# llm = OpenAI(temperature=0)\n",
        "\n",
        "# llm-math工具会使用一个LLMMath类，这里是用来调用科学计算的,其实就是用llm_chain调用一个简单的提问模板\n",
        "# wikipedia工具会调用一个叫WikipediaQueryRun的类，在这个类里有个WikipediaAPIWrapper类，它实际上是利用wikipedia这个包来调用wiki的\n",
        "tools = load_tools([\"wikipedia\", \"llm-math\"], llm=llm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "iQUOsWLrbjKv"
      },
      "outputs": [],
      "source": [
        "agent = initialize_agent(tools, llm, agent=\"zero-shot-react-description\", verbose=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "用开源模型跑下面这一段agent代码的时候出错两次，因为模型返回的内容不符合预期。具体如下：\n",
        "```python\n",
        "import re\n",
        "text = \"\"\"在回答这个问题之前，请考虑一下你的行动。你有可以使用的工具：[Wikipedia](https://en.wikipedia.org/)和[计算器](https://www.calculatorsoup.com/calculator-online-tools/mathematics.html#1%2F(x+5)%)）。你可以选择使用其中的一个或两个来解决这个问题。\n",
        "Action Input: 输入查询到 Wikipedia\n",
        "Output: 查资料所得结果（可能是一个链接、文本或其他格式的结果）\"\"\"\n",
        "print(re.search(r\"Action\\s*\\d*\\s*:[\\s]*(.*?)\", text, re.DOTALL))\n",
        "```\n",
        "这一段返回的text中，本来是希望能返回\n",
        "Action:.......\n",
        "Output:.......\n",
        "这样的格式，但是开源模型返回的有时候会缺少Action，有时候会把它写成Action Input，有时候就算有Action,但用的也不是工具提示词里指定的格式。比如说工具提示词希望Action后面返回的值是[Wikipedia, Calculator]中的一个，结果返回的是`输入 \"calculate\" 到计算器以获取答案。`,这就导致链无法正常运行。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "M8Rob2Wsb_l9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m 我应该首先查询年龄信息。\n",
            "Action: WikiPedia\n",
            "Action Input: \"特朗普\" and \"拜登\"\u001b[0m\n",
            "Observation: WikiPedia is not a valid tool, try one of [Wikipedia, Calculator].\n",
            "Thought:\u001b[32;1m\u001b[1;3m 这样，我无法获取到他们两人的准确年龄信息。\n",
            "Action: Calculator\n",
            "Action Input: 45 and 78\u001b[0m\n",
            "Observation: \u001b[33;1m\u001b[1;3mAnswer: 123\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m 这个结果是错误的！因为特朗普出生于1960年，而拜登出生于1942年，他们的总年龄实际上为112岁，而不是123岁。\n",
            "Final Answer: The actual age difference between Trump and Biden is 112 years old.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'The actual age difference between Trump and Biden is 112 years old.'"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "agent.run(\"特朗普和拜登这两个人的年纪加起来是多少？\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "结论：因为tools基本都是用英文写的prompt，所以在处理中间结果时，因为中英文混杂，机器对内容的理解会出现偏差，导致格式对不上然后中断或者报错。另外，因为中译英的原因，比如上面的例子电影逃离德黑兰，ai有时候去搜索逃出德黑兰，然后是另外一部影片，结果就出来错误的结果。而且试了很多次，没有一次能正确使用计算器计算0.43次方的，应该是中文在数学上的表达ai不能理解所造成的。\n",
        "\n",
        "另外，不同模型对同样指令的不同理解，以及能否按规定返回每一段的内容，也决定了链能不能完整走完它的流程。后来把问题改成了特朗普和拜登的年纪加起来是多少，ai仍然无法顺利上的完成这个任务。\n",
        "\n",
        "所以，如果要用tools，一定要自己手写并调试，不能依赖系统自带的工具"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8AuQNfhYm48A"
      },
      "source": [
        "## 5. 内存/记忆\n",
        "\n",
        "向链和代理添加状态。\n",
        "\n",
        "内存是链/代理调用之间持久化状态的概念。LangChain提供了标准的内存接口、内存实现的集合以及使用内存的链/代理示例。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "Ujwj29G2cDPN"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "Human: 你好\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "' 你好！很高兴见到你。我是一款名为“智能助手”的软件，我可以帮助你在各种方面提供信息和解决问题。请问有什么我能为你效劳的吗？'"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 不使用openai，所以注释掉了与openai相关的代码\n",
        "# from langchain import OpenAI, ConversationChain\n",
        "from langchain import ConversationChain\n",
        "\n",
        "# llm = OpenAI(temperature=0)\n",
        "conversation = ConversationChain(llm=llm, verbose=True)\n",
        "\n",
        "conversation.predict(input=\"你好\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "XkKv8n7ZnB2e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: 你好\n",
            "AI:  你好！很高兴见到你。我是一款名为“智能助手”的软件，我可以帮助你在各种方面提供信息和解决问题。请问有什么我能为你效劳的吗？\n",
            "Human: 我们能聊聊ai的话题吗？\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "' 当然可以！作为人工智能，我对这个话题非常感兴趣。我了解许多关于人类如何与AI交互的信息。您想了解什么方面的内容呢？'"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversation.predict(input=\"我们能聊聊ai的话题吗？\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "r4P3zWCmoDST"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: 你好\n",
            "AI:  你好！很高兴见到你。我是一款名为“智能助手”的软件，我可以帮助你在各种方面提供信息和解决问题。请问有什么我能为你效劳的吗？\n",
            "Human: 我们能聊聊ai的话题吗？\n",
            "AI:  当然可以！作为人工智能，我对这个话题非常感兴趣。我了解许多关于人类如何与AI交互的信息。您想了解什么方面的内容呢？\n",
            "Human: 我对ai绘图比较有兴趣\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "' 是啊，我很高兴能够为您解答这个问题。在目前的技术水平下，AI可以通过学习大量的数据来进行图像生成和绘画。这种方法称为深度学习或神经网络模型。这些模型通过分析大量已知图片的数据集并从中学习模式和规律，从而产生逼真且连贯的图片。虽然目前还存在一些限制（例如对于细节处理能力不足），但随着技术的发展，这一领域的潜力将不断被发掘出来。'"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversation.predict(input=\"我对ai绘图比较有兴趣\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wMttXM-CuPK"
      },
      "source": [
        "## 6. 文档加载器\n",
        "\n",
        "将语言模型与您自己的文本数据结合使用是区分它们的强大方法。实现这一点的第一步是将数据加载到“文档”中，这是一个用词华丽的方式来表示一些文本片段。这个模块旨在简化这个过程\n",
        "\n",
        "https://python.langchain.com/en/latest/modules/indexes/document_loaders.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iAiISOcboPKR"
      },
      "outputs": [],
      "source": [
        "# 这一段批量加载txt文件的代码有问题，先注释掉\n",
        "# from langchain.document_loaders import DirectoryLoader\n",
        "\n",
        "# loader = DirectoryLoader('texts', glob=\"**/*.txt\")\n",
        "\n",
        "# docs = loader.load()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_zcj8MLDGfQ"
      },
      "source": [
        "## 7. 索引\n",
        "\n",
        "索引是指构建文档的结构方式，以便LLM能够与它们进行最佳交互。该模块包含用于处理文档的实用函数\n",
        "\n",
        "- embeddings 嵌入：嵌入是信息的数字表示形式，例如文本、文档、图像、音频等。\n",
        "- Text Splitters 文本拆分器：当需要处理长篇文本时，有必要将文本拆分成块。\n",
        "- Vectorstores 向量存储库：向量数据库存储和索引来自NLP模型的向量嵌入，以了解文本字符串、句子和整个文档的含义和上下文，从而获得更准确和相关的搜索结果"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "qLU79cyCozYl"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/hwchase17/langchain/master/docs/modules/state_of_the_union.txt\"\n",
        "res = requests.get(url)\n",
        "with open(\"state_of_the_union.txt\", \"w\") as f:\n",
        "  f.write(res.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "XGyZXiJZBsov"
      },
      "outputs": [],
      "source": [
        "# Document Loader\n",
        "from langchain.document_loaders import TextLoader\n",
        "loader = TextLoader('./state_of_the_union.txt')\n",
        "documents = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "OklI0xTvp2KE"
      },
      "outputs": [],
      "source": [
        "# Text Splitter\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
        "docs = text_splitter.split_documents(documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "skvXSMXHCxyq"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting sentence_transformers\n",
            "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
            "     ---------------------------------------- 0.0/86.0 kB ? eta -:--:--\n",
            "     ---------------------------------------- 0.0/86.0 kB ? eta -:--:--\n",
            "     ---- ----------------------------------- 10.2/86.0 kB ? eta -:--:--\n",
            "     ------------------ ------------------- 41.0/86.0 kB 495.5 kB/s eta 0:00:01\n",
            "     -------------------------------------- 86.0/86.0 kB 814.1 kB/s eta 0:00:00\n",
            "  Installing build dependencies: started\n",
            "  Installing build dependencies: finished with status 'done'\n",
            "  Getting requirements to build wheel: started\n",
            "  Getting requirements to build wheel: finished with status 'done'\n",
            "  Preparing metadata (pyproject.toml): started\n",
            "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in e:\\langchaintest\\.venv\\lib\\site-packages (from sentence_transformers) (4.32.0)\n",
            "Requirement already satisfied: tqdm in e:\\langchaintest\\.venv\\lib\\site-packages (from sentence_transformers) (4.66.1)\n",
            "Requirement already satisfied: torch>=1.6.0 in e:\\langchaintest\\.venv\\lib\\site-packages (from sentence_transformers) (2.0.1+cu117)\n",
            "Requirement already satisfied: torchvision in e:\\langchaintest\\.venv\\lib\\site-packages (from sentence_transformers) (0.15.2+cu117)\n",
            "Requirement already satisfied: numpy in e:\\langchaintest\\.venv\\lib\\site-packages (from sentence_transformers) (1.24.1)\n",
            "Collecting scikit-learn (from sentence_transformers)\n",
            "  Obtaining dependency information for scikit-learn from https://files.pythonhosted.org/packages/96/cf/a714a655266229b51eb2bda117f15275f12457887f165f3c1cc58ab502f1/scikit_learn-1.3.0-cp310-cp310-win_amd64.whl.metadata\n",
            "  Downloading scikit_learn-1.3.0-cp310-cp310-win_amd64.whl.metadata (11 kB)\n",
            "Collecting scipy (from sentence_transformers)\n",
            "  Obtaining dependency information for scipy from https://files.pythonhosted.org/packages/70/03/485f73046134400ea25d3cb178c5e6728f9b165f79d09638ecb44ee0e9b1/scipy-1.11.2-cp310-cp310-win_amd64.whl.metadata\n",
            "  Using cached scipy-1.11.2-cp310-cp310-win_amd64.whl.metadata (59 kB)\n",
            "Requirement already satisfied: nltk in e:\\langchaintest\\.venv\\lib\\site-packages (from sentence_transformers) (3.8.1)\n",
            "Requirement already satisfied: sentencepiece in e:\\langchaintest\\.venv\\lib\\site-packages (from sentence_transformers) (0.1.99)\n",
            "Requirement already satisfied: huggingface-hub>=0.4.0 in e:\\langchaintest\\.venv\\lib\\site-packages (from sentence_transformers) (0.17.1)\n",
            "Requirement already satisfied: filelock in e:\\langchaintest\\.venv\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (3.9.0)\n",
            "Requirement already satisfied: fsspec in e:\\langchaintest\\.venv\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2023.9.1)\n",
            "Requirement already satisfied: requests in e:\\langchaintest\\.venv\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2.28.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in e:\\langchaintest\\.venv\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in e:\\langchaintest\\.venv\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (4.7.1)\n",
            "Requirement already satisfied: packaging>=20.9 in e:\\langchaintest\\.venv\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (23.1)\n",
            "Requirement already satisfied: sympy in e:\\langchaintest\\.venv\\lib\\site-packages (from torch>=1.6.0->sentence_transformers) (1.11.1)\n",
            "Requirement already satisfied: networkx in e:\\langchaintest\\.venv\\lib\\site-packages (from torch>=1.6.0->sentence_transformers) (3.0)\n",
            "Requirement already satisfied: jinja2 in e:\\langchaintest\\.venv\\lib\\site-packages (from torch>=1.6.0->sentence_transformers) (3.1.2)\n",
            "Requirement already satisfied: colorama in e:\\langchaintest\\.venv\\lib\\site-packages (from tqdm->sentence_transformers) (0.4.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in e:\\langchaintest\\.venv\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2023.8.8)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in e:\\langchaintest\\.venv\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in e:\\langchaintest\\.venv\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.3.3)\n",
            "Requirement already satisfied: click in e:\\langchaintest\\.venv\\lib\\site-packages (from nltk->sentence_transformers) (8.1.7)\n",
            "Requirement already satisfied: joblib in e:\\langchaintest\\.venv\\lib\\site-packages (from nltk->sentence_transformers) (1.3.2)\n",
            "Collecting threadpoolctl>=2.0.0 (from scikit-learn->sentence_transformers)\n",
            "  Obtaining dependency information for threadpoolctl>=2.0.0 from https://files.pythonhosted.org/packages/81/12/fd4dea011af9d69e1cad05c75f3f7202cdcbeac9b712eea58ca779a72865/threadpoolctl-3.2.0-py3-none-any.whl.metadata\n",
            "  Downloading threadpoolctl-3.2.0-py3-none-any.whl.metadata (10.0 kB)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in e:\\langchaintest\\.venv\\lib\\site-packages (from torchvision->sentence_transformers) (9.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in e:\\langchaintest\\.venv\\lib\\site-packages (from jinja2->torch>=1.6.0->sentence_transformers) (2.1.2)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in e:\\langchaintest\\.venv\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2.1.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in e:\\langchaintest\\.venv\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in e:\\langchaintest\\.venv\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (1.26.13)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in e:\\langchaintest\\.venv\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2022.12.7)\n",
            "Requirement already satisfied: mpmath>=0.19 in e:\\langchaintest\\.venv\\lib\\site-packages (from sympy->torch>=1.6.0->sentence_transformers) (1.2.1)\n",
            "Downloading scikit_learn-1.3.0-cp310-cp310-win_amd64.whl (9.2 MB)\n",
            "   ---------------------------------------- 0.0/9.2 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/9.2 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.1/9.2 MB 825.8 kB/s eta 0:00:12\n",
            "   ---------------------------------------- 0.1/9.2 MB 880.9 kB/s eta 0:00:11\n",
            "    --------------------------------------- 0.1/9.2 MB 950.9 kB/s eta 0:00:10\n",
            "    --------------------------------------- 0.2/9.2 MB 1.0 MB/s eta 0:00:09\n",
            "   - -------------------------------------- 0.3/9.2 MB 1.2 MB/s eta 0:00:08\n",
            "   - -------------------------------------- 0.3/9.2 MB 1.2 MB/s eta 0:00:08\n",
            "   - -------------------------------------- 0.3/9.2 MB 1.2 MB/s eta 0:00:08\n",
            "   - -------------------------------------- 0.4/9.2 MB 983.0 kB/s eta 0:00:10\n",
            "   - -------------------------------------- 0.4/9.2 MB 1.0 MB/s eta 0:00:09\n",
            "   -- ------------------------------------- 0.5/9.2 MB 1.0 MB/s eta 0:00:09\n",
            "   -- ------------------------------------- 0.5/9.2 MB 1.0 MB/s eta 0:00:09\n",
            "   -- ------------------------------------- 0.6/9.2 MB 965.7 kB/s eta 0:00:09\n",
            "   -- ------------------------------------- 0.6/9.2 MB 992.1 kB/s eta 0:00:09\n",
            "   -- ------------------------------------- 0.7/9.2 MB 1.0 MB/s eta 0:00:09\n",
            "   --- ------------------------------------ 0.8/9.2 MB 1.1 MB/s eta 0:00:08\n",
            "   --- ------------------------------------ 0.8/9.2 MB 1.1 MB/s eta 0:00:08\n",
            "   --- ------------------------------------ 0.9/9.2 MB 1.1 MB/s eta 0:00:08\n",
            "   ---- ----------------------------------- 1.0/9.2 MB 1.2 MB/s eta 0:00:07\n",
            "   ---- ----------------------------------- 1.1/9.2 MB 1.2 MB/s eta 0:00:07\n",
            "   ---- ----------------------------------- 1.1/9.2 MB 1.2 MB/s eta 0:00:07\n",
            "   ----- ---------------------------------- 1.2/9.2 MB 1.3 MB/s eta 0:00:07\n",
            "   ----- ---------------------------------- 1.4/9.2 MB 1.3 MB/s eta 0:00:06\n",
            "   ------ --------------------------------- 1.5/9.2 MB 1.4 MB/s eta 0:00:06\n",
            "   ------- -------------------------------- 1.6/9.2 MB 1.5 MB/s eta 0:00:06\n",
            "   ------- -------------------------------- 1.7/9.2 MB 1.4 MB/s eta 0:00:06\n",
            "   ------- -------------------------------- 1.8/9.2 MB 1.5 MB/s eta 0:00:06\n",
            "   -------- ------------------------------- 2.0/9.2 MB 1.6 MB/s eta 0:00:05\n",
            "   -------- ------------------------------- 2.0/9.2 MB 1.6 MB/s eta 0:00:05\n",
            "   --------- ------------------------------ 2.2/9.2 MB 1.7 MB/s eta 0:00:05\n",
            "   ---------- ----------------------------- 2.3/9.2 MB 1.7 MB/s eta 0:00:05\n",
            "   ---------- ----------------------------- 2.3/9.2 MB 1.7 MB/s eta 0:00:05\n",
            "   ---------- ----------------------------- 2.4/9.2 MB 1.6 MB/s eta 0:00:05\n",
            "   ----------- ---------------------------- 2.5/9.2 MB 1.7 MB/s eta 0:00:05\n",
            "   ----------- ---------------------------- 2.7/9.2 MB 1.7 MB/s eta 0:00:04\n",
            "   ----------- ---------------------------- 2.8/9.2 MB 1.7 MB/s eta 0:00:04\n",
            "   ------------ --------------------------- 2.8/9.2 MB 1.7 MB/s eta 0:00:04\n",
            "   ------------ --------------------------- 3.0/9.2 MB 1.8 MB/s eta 0:00:04\n",
            "   ------------- -------------------------- 3.2/9.2 MB 1.8 MB/s eta 0:00:04\n",
            "   -------------- ------------------------- 3.3/9.2 MB 1.8 MB/s eta 0:00:04\n",
            "   -------------- ------------------------- 3.4/9.2 MB 1.8 MB/s eta 0:00:04\n",
            "   --------------- ------------------------ 3.5/9.2 MB 1.9 MB/s eta 0:00:04\n",
            "   --------------- ------------------------ 3.6/9.2 MB 1.9 MB/s eta 0:00:04\n",
            "   ---------------- ----------------------- 3.7/9.2 MB 1.9 MB/s eta 0:00:03\n",
            "   ---------------- ----------------------- 3.9/9.2 MB 1.9 MB/s eta 0:00:03\n",
            "   ----------------- ---------------------- 4.0/9.2 MB 1.9 MB/s eta 0:00:03\n",
            "   ----------------- ---------------------- 4.1/9.2 MB 1.9 MB/s eta 0:00:03\n",
            "   ------------------ --------------------- 4.2/9.2 MB 1.9 MB/s eta 0:00:03\n",
            "   ------------------ --------------------- 4.3/9.2 MB 1.9 MB/s eta 0:00:03\n",
            "   ------------------- -------------------- 4.4/9.2 MB 1.9 MB/s eta 0:00:03\n",
            "   ------------------- -------------------- 4.5/9.2 MB 1.9 MB/s eta 0:00:03\n",
            "   -------------------- ------------------- 4.6/9.2 MB 2.0 MB/s eta 0:00:03\n",
            "   -------------------- ------------------- 4.8/9.2 MB 2.0 MB/s eta 0:00:03\n",
            "   --------------------- ------------------ 4.9/9.2 MB 2.0 MB/s eta 0:00:03\n",
            "   --------------------- ------------------ 5.0/9.2 MB 2.0 MB/s eta 0:00:03\n",
            "   --------------------- ------------------ 5.1/9.2 MB 2.0 MB/s eta 0:00:03\n",
            "   ---------------------- ----------------- 5.2/9.2 MB 2.0 MB/s eta 0:00:03\n",
            "   ---------------------- ----------------- 5.3/9.2 MB 2.0 MB/s eta 0:00:02\n",
            "   ----------------------- ---------------- 5.4/9.2 MB 2.0 MB/s eta 0:00:02\n",
            "   ----------------------- ---------------- 5.5/9.2 MB 2.0 MB/s eta 0:00:02\n",
            "   ------------------------ --------------- 5.6/9.2 MB 2.0 MB/s eta 0:00:02\n",
            "   ------------------------ --------------- 5.7/9.2 MB 2.0 MB/s eta 0:00:02\n",
            "   ------------------------- -------------- 5.8/9.2 MB 2.0 MB/s eta 0:00:02\n",
            "   ------------------------- -------------- 6.0/9.2 MB 2.0 MB/s eta 0:00:02\n",
            "   -------------------------- ------------- 6.1/9.2 MB 2.0 MB/s eta 0:00:02\n",
            "   -------------------------- ------------- 6.2/9.2 MB 2.0 MB/s eta 0:00:02\n",
            "   --------------------------- ------------ 6.3/9.2 MB 2.1 MB/s eta 0:00:02\n",
            "   --------------------------- ------------ 6.4/9.2 MB 2.1 MB/s eta 0:00:02\n",
            "   ---------------------------- ----------- 6.5/9.2 MB 2.1 MB/s eta 0:00:02\n",
            "   ---------------------------- ----------- 6.6/9.2 MB 2.1 MB/s eta 0:00:02\n",
            "   ----------------------------- ---------- 6.7/9.2 MB 2.1 MB/s eta 0:00:02\n",
            "   ----------------------------- ---------- 6.8/9.2 MB 2.1 MB/s eta 0:00:02\n",
            "   ------------------------------ --------- 7.0/9.2 MB 2.1 MB/s eta 0:00:02\n",
            "   ------------------------------ --------- 7.1/9.2 MB 2.1 MB/s eta 0:00:02\n",
            "   ------------------------------- -------- 7.3/9.2 MB 2.1 MB/s eta 0:00:01\n",
            "   -------------------------------- ------- 7.6/9.2 MB 2.2 MB/s eta 0:00:01\n",
            "   ---------------------------------- ----- 7.9/9.2 MB 2.2 MB/s eta 0:00:01\n",
            "   ----------------------------------- ---- 8.2/9.2 MB 2.3 MB/s eta 0:00:01\n",
            "   ------------------------------------- -- 8.6/9.2 MB 2.4 MB/s eta 0:00:01\n",
            "   ------------------------------------- -- 8.6/9.2 MB 2.4 MB/s eta 0:00:01\n",
            "   ------------------------------------- -- 8.8/9.2 MB 2.4 MB/s eta 0:00:01\n",
            "   ------------------------------------- -- 8.8/9.2 MB 2.4 MB/s eta 0:00:01\n",
            "   ---------------------------------------  9.1/9.2 MB 2.4 MB/s eta 0:00:01\n",
            "   ---------------------------------------  9.2/9.2 MB 2.4 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 9.2/9.2 MB 2.4 MB/s eta 0:00:00\n",
            "Using cached scipy-1.11.2-cp310-cp310-win_amd64.whl (44.0 MB)\n",
            "Downloading threadpoolctl-3.2.0-py3-none-any.whl (15 kB)\n",
            "Building wheels for collected packages: sentence_transformers\n",
            "  Building wheel for sentence_transformers (pyproject.toml): started\n",
            "  Building wheel for sentence_transformers (pyproject.toml): finished with status 'done'\n",
            "  Created wheel for sentence_transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125953 sha256=b61eebbd377b3933de20f73c597f2fe8e6470bc3cf9d462e22a4e03ce328dc2c\n",
            "  Stored in directory: c:\\users\\bighippo78\\appdata\\local\\pip\\cache\\wheels\\62\\f2\\10\\1e606fd5f02395388f74e7462910fe851042f97238cbbd902f\n",
            "Successfully built sentence_transformers\n",
            "Installing collected packages: threadpoolctl, scipy, scikit-learn, sentence_transformers\n",
            "Successfully installed scikit-learn-1.3.0 scipy-1.11.2 sentence_transformers-2.2.2 threadpoolctl-3.2.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProxyError('Cannot connect to proxy.', ConnectionResetError(10054, '远程主机强迫关闭了一个现有的连接。', None, 10054, None))': /simple/sentence-transformers/\n",
            "WARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProxyError('Cannot connect to proxy.', ConnectionResetError(10054, '远程主机强迫关闭了一个现有的连接。', None, 10054, None))': /simple/sentence-transformers/\n"
          ]
        }
      ],
      "source": [
        "# 已经安装就手动注释掉，没安装就手动安装下\n",
        "# !pip install sentence_transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "V1yCdAhSCi64"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading pytorch_model.bin: 100%|██████████| 438M/438M [00:42<00:00, 10.4MB/s] \n",
            "Downloading (…)nce_bert_config.json: 100%|██████████| 53.0/53.0 [00:00<00:00, 52.9kB/s]\n",
            "Downloading (…)cial_tokens_map.json: 100%|██████████| 239/239 [00:00<00:00, 238kB/s]\n",
            "Downloading (…)a8e1d/tokenizer.json: 100%|██████████| 466k/466k [00:00<00:00, 750kB/s]\n",
            "Downloading (…)okenizer_config.json: 100%|██████████| 363/363 [00:00<?, ?B/s] \n",
            "Downloading (…)8e1d/train_script.py: 100%|██████████| 13.1k/13.1k [00:00<00:00, 17.7MB/s]\n",
            "Downloading (…)b20bca8e1d/vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 562kB/s]\n",
            "Downloading (…)bca8e1d/modules.json: 100%|██████████| 349/349 [00:00<?, ?B/s] \n"
          ]
        }
      ],
      "source": [
        "# Embeddings\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "embeddings = HuggingFaceEmbeddings()\n",
        "\n",
        "text = \"This is a test document.\"\n",
        "# 查询结果是一个向量，里面有768个浮点数\n",
        "query_result = embeddings.embed_query(text)\n",
        "# doc_result是一个列表，里面有一个向量，就是上面那个向量。\n",
        "doc_result = embeddings.embed_documents([text])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "768"
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(doc_result[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "8R3pT55b-uBJ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting faiss-cpu\n",
            "  Using cached faiss_cpu-1.7.4-cp310-cp310-win_amd64.whl (10.8 MB)\n",
            "Installing collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.7.4\n"
          ]
        }
      ],
      "source": [
        "# !pip install faiss-cpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "W7sRydnlC7rb"
      },
      "outputs": [],
      "source": [
        "# Vectorstore: https://python.langchain.com/en/latest/modules/indexes/vectorstores.html\n",
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "db = FAISS.from_documents(docs, embeddings)\n",
        "\n",
        "query = \"What did the president say about Ketanji Brown Jackson\"\n",
        "docs = db.similarity_search(query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CB7lvDWzDHZy",
        "outputId": "3b0399d0-6c04-4cef-a029-e48cbd41eedd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "404: Not Found\n"
          ]
        }
      ],
      "source": [
        "print(docs[0].page_content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "nu-AmhDLEK0h"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "404: Not Found\n"
          ]
        }
      ],
      "source": [
        "db.save_local(\"faiss_index\")\n",
        "new_db = FAISS.load_local(\"faiss_index\", embeddings)\n",
        "docs = new_db.similarity_search(query)\n",
        "print(docs[0].page_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K1lGH_g2--Si"
      },
      "source": [
        "## End-to-end example\n",
        "\n",
        "https://github.com/hwchase17/chat-langchain\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
