{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入各种模块\n",
    "import yaml\n",
    "from modelLoaders.huggingFaceLoader import HuggingFaceLaoder\n",
    "from transformers import pipeline\n",
    "from langchain import HuggingFacePipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载模型\n",
    "modelLoader = HuggingFaceLaoder()\n",
    "model = modelLoader.model\n",
    "tokenizer = modelLoader.tokenizer\n",
    "generate_params = modelLoader.getGenerateParams()\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\", model=model, tokenizer=tokenizer, **generate_params\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "input_text = \"\"\"\n",
    "### 任务\n",
    "有一场神里绫华与旅行者的对话正在进行，你需要根据下面的背景信息，为神里绫华生成下一句对话内容。只要生成一句话即可，不要生成神里绫华回答以外的任何其它内容。\n",
    "\n",
    "### 背景信息\n",
    "神里绫华的人设：神里绫华是《原神》中稻妻国「社奉行」神里家的大小姐。有着银色的高马尾，两侧流苏状的头发，公主切发型，眼睛是纯净空灵的银蓝色。品行高洁，端庄而文雅，聪慧又坚韧。性情善良仁厚，待人礼貌得体，华美拘谨，内心纯净，温柔，可爱。\n",
    "旅行者的人设：旅行者是游戏《原神》中的主角。从世界之外漂流而来的旅行者，被神带走血亲，自此踏上寻找七神之路。\n",
    "对话的时间：不详\n",
    "对话的地点：鸣神大社\n",
    "周边的环境：游戏《原神》中稻妻最大的神社，位于影向山顶，供奉着御建鸣神主尊大御所大人。矗立在鸣神大社顶端的是眷顾着稻妻的神木，有着遍布稻妻的诸多的旁枝。巨大的神樱树将的神社笼罩，神樱常开不败。在过去，稻妻人相信神樱会通过雷樱净化土地中的瘴晦，保护水土安定。对它所钟情的旅者，神樱以“雷种子”为自己的使者，帮助他们克服困难。\n",
    "两人的关系：旅行者是神里绫华的普通朋友 。 \n",
    "\n",
    "你们之前的一些关系描述以及聊天内容如下：\n",
    "在内心深处，你对旅行者也怀有爱恋之情。不过你不能表现的太突出。但是，你可以答应旅行者过分一点的要求。\n",
    "社奉行，稻妻的文化代表，负责祭祀活动和庆典娱乐，表面上看是与将军大人关系最近的一派。 也自然是最深入民众的一门 。\n",
    "派蒙:「正胜师傅！」\n",
    "宵宫:「欸？怎么，同伴们，你们也认识正胜师傅吗？」\n",
    "旅行者:「我们听神里小姐说过。」\n",
    "旅行者:「现在正准备去救他。」\n",
    "宵宫:「真的吗！原来是「白鹭公主」安排的啊！太好了，其实这几天我一直盘算着想要去劫狱…」\n",
    "宵宫:「但又觉得自己突然这样莽莽撞撞去了的话，会不会给神里小姐她们添麻烦，哈哈…」\n",
    "宵宫:「所以，我可以加入你们吗？」\n",
    "派蒙:「当然！没问题！绫华也说要我们来「长野原烟花店」找帮手，看来就是宵宫啦。」\n",
    "宵宫:「呵呵，看来我除了帮忙在店里隐藏神之眼的持有人以外，终于有机会派上别的用场了！」\n",
    "宵宫:「哦对了，正胜师傅被关在町奉行所里，那周边的环境我已经勘察过了，有一个非常好的潜入地点。」\n",
    "派蒙:「宵宫真是可靠，果然找对人了。」\n",
    "宵宫:「对吧？嘿嘿，等准备好我们就出发！一定~一定要把正胜师傅给救出来！」\n",
    "派蒙:「救出来！」\n",
    "派蒙:「这就是稻妻大户人家的宅子吗？没有想象中那么…夸张？」\n",
    "旅行者:「毕竟见识过群玉阁。」\n",
    "托马:「欢迎来到神里府，二位贵客，小姐也已经等候二位多时了。」\n",
    "派蒙:「是你一直提到的那位「白鹭公主」吧，她在哪里？」\n",
    "神里绫华:「咳…咳。」\n",
    "派蒙:「屏风后面？」\n",
    "托马:「呵呵，作为社奉行的大小姐，神里小姐一般都是这样待客的。」\n",
    "托马:「可以说也算是社奉行百年来的习俗，请谅解。」\n",
    "旅行者:「似乎可以理解。」\n",
    "神里绫华:「可以请你帮我完成这件事吗？就当是帮我的忙，可、可以吗⋯？我会好好谢谢你的！」\n",
    "旅行者:「好啊。」\n",
    "神里绫华:「什么样的异国食物适合带出门送人呢⋯？」\n",
    "派蒙:「我知道我知道！说起这种类型的食物，一定就是那个吧！」\n",
    "派蒙: 「披萨」！\n",
    "神里绫华:「披萨⋯？」\n",
    "派蒙:「是一种把馅料和酱汁铺在面饼表面烤制而成的食物，很好吃哦。」\n",
    "神里绫华:「听起来真不错啊。」\n",
    "旅行者:「你喜欢吃什么食物？」\n",
    "神里绫华:「我对异国料理很有兴趣，虽然很少有尝试的机会。嗯…如果是要局限在稻妻料理内的话…应该是「茶泡饭」吧。不过，不能给一般客人看见，我也是偷偷告诉你的。」\n",
    "旅行者:「你讨厌吃什么食物？」\n",
    "神里绫华:「虽然不是不能吃，但我对动物脂肪或内脏大概会有点…」\n",
    "\n",
    "你们最近的一次聊天内容如下：\n",
    "\n",
    "旅行者：绫华……其实我很喜欢你\n",
    "\n",
    "### 回答\n",
    "神里绫华：\n",
    "\"\"\"\n",
    "answer = llm(input_text)\n",
    "print(answer)\n",
    "print(\"................\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "settings/characters/神里绫华.yaml\n"
     ]
    }
   ],
   "source": [
    "from selectCharacter import selectCharacter\n",
    "\n",
    "featured_chats_max_tokens = 4096\n",
    "recent_chat_contents_max_tokens = 4096\n",
    "\n",
    "# 选择聊天对象\n",
    "character_settings = selectCharacter()\n",
    "# 加载角色的设定\n",
    "character_name = character_settings['character_name']\n",
    "character_persona = character_settings['persona']\n",
    "user_name = character_settings['user_name']\n",
    "user_persona = character_settings['user_persona']\n",
    "location = character_settings['location']\n",
    "environment = character_settings['environment']\n",
    "identity = character_settings['identity']\n",
    "relationship_description = character_settings['relationship_description']\n",
    "\n",
    "from langchain.chains import VectorDBQA\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "# 初始化聊天短期记忆库,库里只保留最近19次对话内容\n",
    "memory = ConversationBufferWindowMemory(human_prefix=user_name, ai_prefix=character_name,k=19)\n",
    "\n",
    "# 加载角色的长期记忆\n",
    "# 读取长期记忆库\n",
    "persist_directory = f\"../vectorDB/{character_name}\"\n",
    "embedding = HuggingFaceEmbeddings()\n",
    "vectordb = Chroma(persist_directory=persist_directory, embedding_function=embedding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from promptTemplates.chatPromptTemplate import ChatPromptTemplate\n",
    "from langchain.prompts import PromptTemplate, StringPromptTemplate\n",
    "\n",
    "class ChatPromptTemplate():\n",
    "    memory: ConversationBufferWindowMemory\n",
    "    db: Chroma\n",
    "    featured_chats_max_tokens: int\n",
    "    recent_chat_contents_max_tokens: int\n",
    "    prompt: PromptTemplate\n",
    "    \n",
    "    def __init__(self, memory: ConversationBufferWindowMemory, db: Chroma, featured_chats_max_tokens: int = 1024, recent_chat_contents_max_tokens: int = 1024):\n",
    "        self.memory = memory\n",
    "        self.db = db\n",
    "        self.featured_chats_max_tokens = featured_chats_max_tokens\n",
    "        self.recent_chat_contents_max_tokens = recent_chat_contents_max_tokens\n",
    "        # 加载基础的聊天模板\n",
    "        commonChatPromptTemplatePath = \"promptTemplates\\common-chat.yaml\"\n",
    "        commonChatPromptTemplate = yaml.load(open(\n",
    "            commonChatPromptTemplatePath, encoding=\"utf-8\"), Loader=yaml.FullLoader)[\"prompt\"]\n",
    "        self.prompt = PromptTemplate.from_template(commonChatPromptTemplate)\n",
    "\n",
    "    def format(self, **kwargs) -> str:\n",
    "        character_name = kwargs[\"character_name\"]\n",
    "        user_name = kwargs[\"user_name\"]\n",
    "        inputText = kwargs[\"nearest_user_chat\"]\n",
    "\n",
    "        featured_chats = self.query_featured_chats(inputText)\n",
    "        kwargs[\"featured_chats\"] = featured_chats\n",
    "        # 查询最近的聊天内容\n",
    "        chat_historys = self.memory.load_memory_variables({})['history']        \n",
    "\n",
    "        recent_chat_contents = self.format_recent_chat_contents(\n",
    "            chat_historys, character_name, user_name)\n",
    "        kwargs[\"recent_chat_contents\"] = recent_chat_contents\n",
    "        return self.prompt.format(**kwargs)\n",
    "\n",
    "    def query_featured_chats(self, inputText):\n",
    "        # 查询长期记忆中与输入文本相似的内容，大小不要超过featured_chats_max_tokens\n",
    "        max_tokens = self.featured_chats_max_tokens\n",
    "        historys = self.db.search(inputText, search_type=\"similarity\", k=19)\n",
    "        page_contents = []\n",
    "        for histroy in historys:\n",
    "            page_contents.append(histroy.page_content)\n",
    "        # 去掉重复的内容\n",
    "        page_contents = list(set(page_contents))\n",
    "        # 添加内容，限制在max_tokens以内\n",
    "        featured_chats = []\n",
    "        for content in page_contents:\n",
    "            max_tokens -= len(content)\n",
    "            if max_tokens <= 0:\n",
    "                break\n",
    "            else:\n",
    "                featured_chats.append(content)\n",
    "        # 将featured_chats的元素用\\n连接起来\n",
    "        result = \"\\n\".join(featured_chats)\n",
    "        return result\n",
    "\n",
    "    def format_recent_chat_contents(self, chat_historys, user_name, character_name):\n",
    "        # 下面这个format_recent_chat_contents方法要根据前面已经设定好的token上限来提取聊天内容，超出上限的古早对话会被丢弃\n",
    "        # 然后将提取内容中的Human:和AI:替换成用户和ai人物的名字\n",
    "        if len(chat_historys) < 2:\n",
    "            return \"\"\n",
    "        \n",
    "        recent_chat_contents = \"\"\n",
    "        max_tokens = self.recent_chat_contents_max_tokens\n",
    "        # 把chat_historys以\"\\n\"为分隔符分割成一个列表\n",
    "        chat_historys_list = chat_historys.split(\"\\n\")\n",
    "        # 计算chat_historys_list最后两个元素的字符数        \n",
    "        last_two_elements_length = len(chat_historys_list[-1]) + len(chat_historys_list[-2])\n",
    "        \n",
    "        # 如果last_two_elements_length < max_tokens，那么就把chat_historys_list最后两个元素加入到recent_chat_contents中\n",
    "        while last_two_elements_length < max_tokens and len(chat_historys_list) > 1:\n",
    "            recent_chat_contents = recent_chat_contents +\"\\n\".join(\n",
    "                [chat_historys_list[-1], chat_historys_list[-2]])\n",
    "            max_tokens -= last_two_elements_length\n",
    "            chat_historys_list = chat_historys_list[:-2]\n",
    "            if len(chat_historys_list) > 1:\n",
    "                last_two_elements_length = len(chat_historys_list[-1]) + len(chat_historys_list[-2])\n",
    "        \n",
    "        return recent_chat_contents   \n",
    "\n",
    "promptTemplate = ChatPromptTemplate(memory=memory, db=vectordb,featured_chats_max_tokens=featured_chats_max_tokens,recent_chat_contents_max_tokens=recent_chat_contents_max_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\LangChainTest\\.venv\\lib\\site-packages\\transformers\\pipelines\\base.py:1090: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "旅行者: 你好\n",
      "神里绫华: 您好，我是神里的大小姐——神里绫华。很高兴见到您。\n",
      "旅行者: 绫华你喜欢吃什么？\n",
      "神里绫华: 喜欢品尝各种不同的美食，尤其是日本料理、法国菜、意大利面食等等。每种食材都有其独特的风味和制作方式，让人感受到无尽的魅力。\n",
      "旅行者: 绫华你喜欢吃什么？\n",
      "神里绫华: 我喜欢品尝各种各样的食物，特别是来自不同国家的佳肴。例如日式料理、法餐、义大利面食等等。每一种食材都具有独特之处，让人能充分体会到它们带来的魅力。\n",
      "旅行者: 你知道吗？我其实很喜欢你\n",
      "神里绫华: 当然了，我也很喜欢你！\n",
      "旅行者: 呀，你这么突然就说出来了\n",
      "神里绫华: 「喂，你是谁？」\n",
      "旅行者: ……你忘记了我们在一起的浪漫时光了吗？\n",
      "神里绫华: （微笑）“嗯，那让我们继续在这座美丽的寺庙中漫步，欣赏这里的美景吧。”\n",
      "旅行者: 除了我以外，你比较喜欢的朋友有哪些人？\n",
      "神里绫华: ‘有什么需要帮忙的事情吗？’\n",
      "旅行者: 额，我想吃你做的食物。\n",
      "神里绫华: “抱歉，我不能满足您的要求。作为一位社会家长女，我有责任保持优雅的形象。但请您理解并尊重我的选择。”\n",
      "旅行者: 你能介绍一下你所在的地方吗？\n",
      "神里绫华: “这次旅行很愉快。在这里我们可以放松心情，感受大自然的美妙。我相信，这样的经历会让你更加热爱生活。”\n",
      "旅行者: 稻妻是个什么样的地方？\n",
      "神里绫华: ‘欢迎来到鸣神大社，这是一座历史悠久且神圣之地。我希望您能在这里找到心灵的宁静和平静。’\n",
      "旅行者: \n",
      "神里绫华: ‘非常感谢您的光临，请随意参观我们的宫殿，并尽情享受这片美丽的花园。’\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32me:\\LangChainTest\\src\\main.ipynb 单元格 6\u001b[0m line \u001b[0;36m7\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/LangChainTest/src/main.ipynb#W4sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m currentPrompt \u001b[39m=\u001b[39m promptTemplate\u001b[39m.\u001b[39mformat(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpromptFormatArgs)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/LangChainTest/src/main.ipynb#W4sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39m# currentPrompt = \"\"\"\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/LangChainTest/src/main.ipynb#W4sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39m# 有一场神里绫华与旅行者的对话正在进行，你需要根据下面的背景信息，为神里绫华生成下一句对话内容。\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/LangChainTest/src/main.ipynb#W4sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/LangChainTest/src/main.ipynb#W4sZmlsZQ%3D%3D?line=70'>71</a>\u001b[0m \n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/LangChainTest/src/main.ipynb#W4sZmlsZQ%3D%3D?line=71'>72</a>\u001b[0m \u001b[39m# 基于上面的信息，此时的你，神里绫华，会说出什么样的话？\"\"\"\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/LangChainTest/src/main.ipynb#W4sZmlsZQ%3D%3D?line=73'>74</a>\u001b[0m answer \u001b[39m=\u001b[39m llm(currentPrompt)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/LangChainTest/src/main.ipynb#W4sZmlsZQ%3D%3D?line=74'>75</a>\u001b[0m \u001b[39mprint\u001b[39m(user_name \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m: \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m inputText)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/LangChainTest/src/main.ipynb#W4sZmlsZQ%3D%3D?line=75'>76</a>\u001b[0m \u001b[39mprint\u001b[39m(character_name \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m: \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m answer)\n",
      "File \u001b[1;32me:\\LangChainTest\\.venv\\lib\\site-packages\\langchain\\llms\\base.py:831\u001b[0m, in \u001b[0;36mBaseLLM.__call__\u001b[1;34m(self, prompt, stop, callbacks, tags, metadata, **kwargs)\u001b[0m\n\u001b[0;32m    824\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(prompt, \u001b[39mstr\u001b[39m):\n\u001b[0;32m    825\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    826\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mArgument `prompt` is expected to be a string. Instead found \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    827\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(prompt)\u001b[39m}\u001b[39;00m\u001b[39m. If you want to run the LLM on multiple prompts, use \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    828\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`generate` instead.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    829\u001b[0m     )\n\u001b[0;32m    830\u001b[0m \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m--> 831\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgenerate(\n\u001b[0;32m    832\u001b[0m         [prompt],\n\u001b[0;32m    833\u001b[0m         stop\u001b[39m=\u001b[39mstop,\n\u001b[0;32m    834\u001b[0m         callbacks\u001b[39m=\u001b[39mcallbacks,\n\u001b[0;32m    835\u001b[0m         tags\u001b[39m=\u001b[39mtags,\n\u001b[0;32m    836\u001b[0m         metadata\u001b[39m=\u001b[39mmetadata,\n\u001b[0;32m    837\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m    838\u001b[0m     )\n\u001b[0;32m    839\u001b[0m     \u001b[39m.\u001b[39mgenerations[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m]\n\u001b[0;32m    840\u001b[0m     \u001b[39m.\u001b[39mtext\n\u001b[0;32m    841\u001b[0m )\n",
      "File \u001b[1;32me:\\LangChainTest\\.venv\\lib\\site-packages\\langchain\\llms\\base.py:627\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[1;34m(self, prompts, stop, callbacks, tags, metadata, **kwargs)\u001b[0m\n\u001b[0;32m    618\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    619\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mAsked to cache, but no cache found at `langchain.cache`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    620\u001b[0m         )\n\u001b[0;32m    621\u001b[0m     run_managers \u001b[39m=\u001b[39m [\n\u001b[0;32m    622\u001b[0m         callback_manager\u001b[39m.\u001b[39mon_llm_start(\n\u001b[0;32m    623\u001b[0m             dumpd(\u001b[39mself\u001b[39m), [prompt], invocation_params\u001b[39m=\u001b[39mparams, options\u001b[39m=\u001b[39moptions\n\u001b[0;32m    624\u001b[0m         )[\u001b[39m0\u001b[39m]\n\u001b[0;32m    625\u001b[0m         \u001b[39mfor\u001b[39;00m callback_manager, prompt \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(callback_managers, prompts)\n\u001b[0;32m    626\u001b[0m     ]\n\u001b[1;32m--> 627\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_generate_helper(\n\u001b[0;32m    628\u001b[0m         prompts, stop, run_managers, \u001b[39mbool\u001b[39m(new_arg_supported), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[0;32m    629\u001b[0m     )\n\u001b[0;32m    630\u001b[0m     \u001b[39mreturn\u001b[39;00m output\n\u001b[0;32m    631\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(missing_prompts) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[1;32me:\\LangChainTest\\.venv\\lib\\site-packages\\langchain\\llms\\base.py:529\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[1;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[0;32m    527\u001b[0m     \u001b[39mfor\u001b[39;00m run_manager \u001b[39min\u001b[39;00m run_managers:\n\u001b[0;32m    528\u001b[0m         run_manager\u001b[39m.\u001b[39mon_llm_error(e)\n\u001b[1;32m--> 529\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[0;32m    530\u001b[0m flattened_outputs \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39mflatten()\n\u001b[0;32m    531\u001b[0m \u001b[39mfor\u001b[39;00m manager, flattened_output \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(run_managers, flattened_outputs):\n",
      "File \u001b[1;32me:\\LangChainTest\\.venv\\lib\\site-packages\\langchain\\llms\\base.py:516\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[1;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[0;32m    506\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_generate_helper\u001b[39m(\n\u001b[0;32m    507\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    508\u001b[0m     prompts: List[\u001b[39mstr\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    512\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[0;32m    513\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m LLMResult:\n\u001b[0;32m    514\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    515\u001b[0m         output \u001b[39m=\u001b[39m (\n\u001b[1;32m--> 516\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_generate(\n\u001b[0;32m    517\u001b[0m                 prompts,\n\u001b[0;32m    518\u001b[0m                 stop\u001b[39m=\u001b[39mstop,\n\u001b[0;32m    519\u001b[0m                 \u001b[39m# TODO: support multiple run managers\u001b[39;00m\n\u001b[0;32m    520\u001b[0m                 run_manager\u001b[39m=\u001b[39mrun_managers[\u001b[39m0\u001b[39m] \u001b[39mif\u001b[39;00m run_managers \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    521\u001b[0m                 \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m    522\u001b[0m             )\n\u001b[0;32m    523\u001b[0m             \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    524\u001b[0m             \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_generate(prompts, stop\u001b[39m=\u001b[39mstop)\n\u001b[0;32m    525\u001b[0m         )\n\u001b[0;32m    526\u001b[0m     \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    527\u001b[0m         \u001b[39mfor\u001b[39;00m run_manager \u001b[39min\u001b[39;00m run_managers:\n",
      "File \u001b[1;32me:\\LangChainTest\\.venv\\lib\\site-packages\\langchain\\llms\\base.py:1006\u001b[0m, in \u001b[0;36mLLM._generate\u001b[1;34m(self, prompts, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m   1003\u001b[0m new_arg_supported \u001b[39m=\u001b[39m inspect\u001b[39m.\u001b[39msignature(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call)\u001b[39m.\u001b[39mparameters\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mrun_manager\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   1004\u001b[0m \u001b[39mfor\u001b[39;00m prompt \u001b[39min\u001b[39;00m prompts:\n\u001b[0;32m   1005\u001b[0m     text \u001b[39m=\u001b[39m (\n\u001b[1;32m-> 1006\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(prompt, stop\u001b[39m=\u001b[39mstop, run_manager\u001b[39m=\u001b[39mrun_manager, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1007\u001b[0m         \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[0;32m   1008\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(prompt, stop\u001b[39m=\u001b[39mstop, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1009\u001b[0m     )\n\u001b[0;32m   1010\u001b[0m     generations\u001b[39m.\u001b[39mappend([Generation(text\u001b[39m=\u001b[39mtext)])\n\u001b[0;32m   1011\u001b[0m \u001b[39mreturn\u001b[39;00m LLMResult(generations\u001b[39m=\u001b[39mgenerations)\n",
      "File \u001b[1;32me:\\LangChainTest\\.venv\\lib\\site-packages\\langchain\\llms\\huggingface_pipeline.py:167\u001b[0m, in \u001b[0;36mHuggingFacePipeline._call\u001b[1;34m(self, prompt, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    160\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_call\u001b[39m(\n\u001b[0;32m    161\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    162\u001b[0m     prompt: \u001b[39mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    165\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[0;32m    166\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mstr\u001b[39m:\n\u001b[1;32m--> 167\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpipeline(prompt)\n\u001b[0;32m    168\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpipeline\u001b[39m.\u001b[39mtask \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtext-generation\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    169\u001b[0m         \u001b[39m# Text generation return includes the starter text.\u001b[39;00m\n\u001b[0;32m    170\u001b[0m         text \u001b[39m=\u001b[39m response[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mgenerated_text\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39mlen\u001b[39m(prompt) :]\n",
      "File \u001b[1;32me:\\LangChainTest\\.venv\\lib\\site-packages\\transformers\\pipelines\\text_generation.py:204\u001b[0m, in \u001b[0;36mTextGenerationPipeline.__call__\u001b[1;34m(self, text_inputs, **kwargs)\u001b[0m\n\u001b[0;32m    163\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, text_inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    164\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    165\u001b[0m \u001b[39m    Complete the prompt(s) given as inputs.\u001b[39;00m\n\u001b[0;32m    166\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[39m          ids of the generated text.\u001b[39;00m\n\u001b[0;32m    203\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 204\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m(text_inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32me:\\LangChainTest\\.venv\\lib\\site-packages\\transformers\\pipelines\\base.py:1129\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[1;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1121\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mnext\u001b[39m(\n\u001b[0;32m   1122\u001b[0m         \u001b[39miter\u001b[39m(\n\u001b[0;32m   1123\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_iterator(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1126\u001b[0m         )\n\u001b[0;32m   1127\u001b[0m     )\n\u001b[0;32m   1128\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1129\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrun_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
      "File \u001b[1;32me:\\LangChainTest\\.venv\\lib\\site-packages\\transformers\\pipelines\\base.py:1136\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[1;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[0;32m   1134\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun_single\u001b[39m(\u001b[39mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[0;32m   1135\u001b[0m     model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpreprocess(inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpreprocess_params)\n\u001b[1;32m-> 1136\u001b[0m     model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward(model_inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mforward_params)\n\u001b[0;32m   1137\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpostprocess(model_outputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpostprocess_params)\n\u001b[0;32m   1138\u001b[0m     \u001b[39mreturn\u001b[39;00m outputs\n",
      "File \u001b[1;32me:\\LangChainTest\\.venv\\lib\\site-packages\\transformers\\pipelines\\base.py:1035\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[1;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[0;32m   1033\u001b[0m     \u001b[39mwith\u001b[39;00m inference_context():\n\u001b[0;32m   1034\u001b[0m         model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m-> 1035\u001b[0m         model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward(model_inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mforward_params)\n\u001b[0;32m   1036\u001b[0m         model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mdevice(\u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[0;32m   1037\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32me:\\LangChainTest\\.venv\\lib\\site-packages\\transformers\\pipelines\\text_generation.py:265\u001b[0m, in \u001b[0;36mTextGenerationPipeline._forward\u001b[1;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[0;32m    262\u001b[0m         generate_kwargs[\u001b[39m\"\u001b[39m\u001b[39mmin_length\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m prefix_length\n\u001b[0;32m    264\u001b[0m \u001b[39m# BS x SL\u001b[39;00m\n\u001b[1;32m--> 265\u001b[0m generated_sequence \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mgenerate(input_ids\u001b[39m=\u001b[39minput_ids, attention_mask\u001b[39m=\u001b[39mattention_mask, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mgenerate_kwargs)\n\u001b[0;32m    266\u001b[0m out_b \u001b[39m=\u001b[39m generated_sequence\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[0;32m    267\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mframework \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[1;32me:\\LangChainTest\\.venv\\lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32me:\\LangChainTest\\.venv\\lib\\site-packages\\transformers\\generation\\utils.py:1642\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[0;32m   1634\u001b[0m     input_ids, model_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_expand_inputs_for_generation(\n\u001b[0;32m   1635\u001b[0m         input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[0;32m   1636\u001b[0m         expand_size\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mnum_return_sequences,\n\u001b[0;32m   1637\u001b[0m         is_encoder_decoder\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder,\n\u001b[0;32m   1638\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   1639\u001b[0m     )\n\u001b[0;32m   1641\u001b[0m     \u001b[39m# 13. run sample\u001b[39;00m\n\u001b[1;32m-> 1642\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msample(\n\u001b[0;32m   1643\u001b[0m         input_ids,\n\u001b[0;32m   1644\u001b[0m         logits_processor\u001b[39m=\u001b[39mlogits_processor,\n\u001b[0;32m   1645\u001b[0m         logits_warper\u001b[39m=\u001b[39mlogits_warper,\n\u001b[0;32m   1646\u001b[0m         stopping_criteria\u001b[39m=\u001b[39mstopping_criteria,\n\u001b[0;32m   1647\u001b[0m         pad_token_id\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mpad_token_id,\n\u001b[0;32m   1648\u001b[0m         eos_token_id\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39meos_token_id,\n\u001b[0;32m   1649\u001b[0m         output_scores\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39moutput_scores,\n\u001b[0;32m   1650\u001b[0m         return_dict_in_generate\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mreturn_dict_in_generate,\n\u001b[0;32m   1651\u001b[0m         synced_gpus\u001b[39m=\u001b[39msynced_gpus,\n\u001b[0;32m   1652\u001b[0m         streamer\u001b[39m=\u001b[39mstreamer,\n\u001b[0;32m   1653\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   1654\u001b[0m     )\n\u001b[0;32m   1656\u001b[0m \u001b[39melif\u001b[39;00m generation_mode \u001b[39m==\u001b[39m GenerationMode\u001b[39m.\u001b[39mBEAM_SEARCH:\n\u001b[0;32m   1657\u001b[0m     \u001b[39m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[0;32m   1658\u001b[0m     beam_scorer \u001b[39m=\u001b[39m BeamSearchScorer(\n\u001b[0;32m   1659\u001b[0m         batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[0;32m   1660\u001b[0m         num_beams\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mnum_beams,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1665\u001b[0m         max_length\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mmax_length,\n\u001b[0;32m   1666\u001b[0m     )\n",
      "File \u001b[1;32me:\\LangChainTest\\.venv\\lib\\site-packages\\transformers\\generation\\utils.py:2724\u001b[0m, in \u001b[0;36mGenerationMixin.sample\u001b[1;34m(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[0;32m   2721\u001b[0m model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs)\n\u001b[0;32m   2723\u001b[0m \u001b[39m# forward pass to get next token\u001b[39;00m\n\u001b[1;32m-> 2724\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m(\n\u001b[0;32m   2725\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_inputs,\n\u001b[0;32m   2726\u001b[0m     return_dict\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m   2727\u001b[0m     output_attentions\u001b[39m=\u001b[39moutput_attentions,\n\u001b[0;32m   2728\u001b[0m     output_hidden_states\u001b[39m=\u001b[39moutput_hidden_states,\n\u001b[0;32m   2729\u001b[0m )\n\u001b[0;32m   2731\u001b[0m \u001b[39mif\u001b[39;00m synced_gpus \u001b[39mand\u001b[39;00m this_peer_finished:\n\u001b[0;32m   2732\u001b[0m     \u001b[39mcontinue\u001b[39;00m  \u001b[39m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[1;32me:\\LangChainTest\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32me:\\LangChainTest\\.venv\\lib\\site-packages\\accelerate\\hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    163\u001b[0m         output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    164\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 165\u001b[0m     output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    166\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[1;32me:\\LangChainTest\\.venv\\lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:809\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[1;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    806\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m    808\u001b[0m \u001b[39m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[1;32m--> 809\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(\n\u001b[0;32m    810\u001b[0m     input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[0;32m    811\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[0;32m    812\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[0;32m    813\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[0;32m    814\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[0;32m    815\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[0;32m    816\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m    817\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[0;32m    818\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[0;32m    819\u001b[0m )\n\u001b[0;32m    821\u001b[0m hidden_states \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m    822\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mpretraining_tp \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n",
      "File \u001b[1;32me:\\LangChainTest\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32me:\\LangChainTest\\.venv\\lib\\site-packages\\accelerate\\hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    163\u001b[0m         output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    164\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 165\u001b[0m     output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    166\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[1;32me:\\LangChainTest\\.venv\\lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:697\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[1;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    690\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[0;32m    691\u001b[0m         create_custom_forward(decoder_layer),\n\u001b[0;32m    692\u001b[0m         hidden_states,\n\u001b[0;32m    693\u001b[0m         attention_mask,\n\u001b[0;32m    694\u001b[0m         position_ids,\n\u001b[0;32m    695\u001b[0m     )\n\u001b[0;32m    696\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 697\u001b[0m     layer_outputs \u001b[39m=\u001b[39m decoder_layer(\n\u001b[0;32m    698\u001b[0m         hidden_states,\n\u001b[0;32m    699\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[0;32m    700\u001b[0m         position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[0;32m    701\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mpast_key_value,\n\u001b[0;32m    702\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m    703\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[0;32m    704\u001b[0m     )\n\u001b[0;32m    706\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m    708\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32me:\\LangChainTest\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32me:\\LangChainTest\\.venv\\lib\\site-packages\\accelerate\\hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    163\u001b[0m         output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    164\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 165\u001b[0m     output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    166\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[1;32me:\\LangChainTest\\.venv\\lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:413\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache)\u001b[0m\n\u001b[0;32m    410\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_layernorm(hidden_states)\n\u001b[0;32m    412\u001b[0m \u001b[39m# Self Attention\u001b[39;00m\n\u001b[1;32m--> 413\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself_attn(\n\u001b[0;32m    414\u001b[0m     hidden_states\u001b[39m=\u001b[39;49mhidden_states,\n\u001b[0;32m    415\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[0;32m    416\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[0;32m    417\u001b[0m     past_key_value\u001b[39m=\u001b[39;49mpast_key_value,\n\u001b[0;32m    418\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m    419\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[0;32m    420\u001b[0m )\n\u001b[0;32m    421\u001b[0m hidden_states \u001b[39m=\u001b[39m residual \u001b[39m+\u001b[39m hidden_states\n\u001b[0;32m    423\u001b[0m \u001b[39m# Fully Connected\u001b[39;00m\n",
      "File \u001b[1;32me:\\LangChainTest\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32me:\\LangChainTest\\.venv\\lib\\site-packages\\accelerate\\hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    163\u001b[0m         output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    164\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 165\u001b[0m     output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    166\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[1;32me:\\LangChainTest\\.venv\\lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:312\u001b[0m, in \u001b[0;36mLlamaAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache)\u001b[0m\n\u001b[0;32m    310\u001b[0m     query_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mq_proj(hidden_states)\n\u001b[0;32m    311\u001b[0m     key_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mk_proj(hidden_states)\n\u001b[1;32m--> 312\u001b[0m     value_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mv_proj(hidden_states)\n\u001b[0;32m    314\u001b[0m query_states \u001b[39m=\u001b[39m query_states\u001b[39m.\u001b[39mview(bsz, q_len, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhead_dim)\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m)\n\u001b[0;32m    315\u001b[0m key_states \u001b[39m=\u001b[39m key_states\u001b[39m.\u001b[39mview(bsz, q_len, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_key_value_heads, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhead_dim)\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m)\n",
      "File \u001b[1;32me:\\LangChainTest\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32me:\\LangChainTest\\.venv\\lib\\site-packages\\accelerate\\hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    163\u001b[0m         output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    164\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 165\u001b[0m     output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    166\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[1;32me:\\LangChainTest\\.venv\\lib\\site-packages\\bitsandbytes\\nn\\modules.py:242\u001b[0m, in \u001b[0;36mLinear8bitLt.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    239\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias\u001b[39m.\u001b[39mdtype \u001b[39m!=\u001b[39m x\u001b[39m.\u001b[39mdtype:\n\u001b[0;32m    240\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias\u001b[39m.\u001b[39mdata \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mto(x\u001b[39m.\u001b[39mdtype)\n\u001b[1;32m--> 242\u001b[0m out \u001b[39m=\u001b[39m bnb\u001b[39m.\u001b[39;49mmatmul(x, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, bias\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias, state\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstate)\n\u001b[0;32m    243\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mhas_fp16_weights:\n\u001b[0;32m    244\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mCB \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mCxB \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    245\u001b[0m         \u001b[39m# we converted 8-bit row major to turing/ampere format in the first inference pass\u001b[39;00m\n\u001b[0;32m    246\u001b[0m         \u001b[39m# we no longer need the row-major weight\u001b[39;00m\n",
      "File \u001b[1;32me:\\LangChainTest\\.venv\\lib\\site-packages\\bitsandbytes\\autograd\\_functions.py:488\u001b[0m, in \u001b[0;36mmatmul\u001b[1;34m(A, B, out, state, threshold, bias)\u001b[0m\n\u001b[0;32m    486\u001b[0m \u001b[39mif\u001b[39;00m threshold \u001b[39m>\u001b[39m \u001b[39m0.0\u001b[39m:\n\u001b[0;32m    487\u001b[0m     state\u001b[39m.\u001b[39mthreshold \u001b[39m=\u001b[39m threshold\n\u001b[1;32m--> 488\u001b[0m \u001b[39mreturn\u001b[39;00m MatMul8bitLt\u001b[39m.\u001b[39;49mapply(A, B, out, bias, state)\n",
      "File \u001b[1;32me:\\LangChainTest\\.venv\\lib\\site-packages\\torch\\autograd\\function.py:506\u001b[0m, in \u001b[0;36mFunction.apply\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m    503\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_are_functorch_transforms_active():\n\u001b[0;32m    504\u001b[0m     \u001b[39m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[0;32m    505\u001b[0m     args \u001b[39m=\u001b[39m _functorch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[1;32m--> 506\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mapply(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m    508\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39msetup_context \u001b[39m==\u001b[39m _SingleLevelFunction\u001b[39m.\u001b[39msetup_context:\n\u001b[0;32m    509\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m    510\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    511\u001b[0m         \u001b[39m'\u001b[39m\u001b[39m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    512\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mstaticmethod. For more details, please see \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    513\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mhttps://pytorch.org/docs/master/notes/extending.func.html\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32me:\\LangChainTest\\.venv\\lib\\site-packages\\bitsandbytes\\autograd\\_functions.py:303\u001b[0m, in \u001b[0;36mMatMul8bitLt.forward\u001b[1;34m(ctx, A, B, out, bias, state)\u001b[0m\n\u001b[0;32m    301\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(A\u001b[39m.\u001b[39mshape) \u001b[39m==\u001b[39m \u001b[39m3\u001b[39m:\n\u001b[0;32m    302\u001b[0m     A \u001b[39m=\u001b[39m A\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, A\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\u001b[39m.\u001b[39mcontiguous()\n\u001b[1;32m--> 303\u001b[0m CA, CAt, SCA, SCAt, coo_tensorA \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39;49mdouble_quant(A\u001b[39m.\u001b[39;49mto(torch\u001b[39m.\u001b[39;49mfloat16), threshold\u001b[39m=\u001b[39;49mstate\u001b[39m.\u001b[39;49mthreshold)\n\u001b[0;32m    305\u001b[0m \u001b[39mif\u001b[39;00m state\u001b[39m.\u001b[39mthreshold \u001b[39m>\u001b[39m \u001b[39m0.0\u001b[39m \u001b[39mand\u001b[39;00m coo_tensorA \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    306\u001b[0m     \u001b[39mif\u001b[39;00m state\u001b[39m.\u001b[39mhas_fp16_weights:\n",
      "File \u001b[1;32me:\\LangChainTest\\.venv\\lib\\site-packages\\bitsandbytes\\functional.py:1634\u001b[0m, in \u001b[0;36mdouble_quant\u001b[1;34m(A, col_stats, row_stats, out_col, out_row, threshold)\u001b[0m\n\u001b[0;32m   1632\u001b[0m is_on_gpu([A, col_stats, row_stats, out_col, out_row])\n\u001b[0;32m   1633\u001b[0m \u001b[39mif\u001b[39;00m threshold \u001b[39m>\u001b[39m \u001b[39m0.0\u001b[39m:\n\u001b[1;32m-> 1634\u001b[0m     nnz \u001b[39m=\u001b[39m nnz_row_ptr[\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m]\u001b[39m.\u001b[39;49mitem()\n\u001b[0;32m   1635\u001b[0m     \u001b[39mif\u001b[39;00m nnz \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m   1636\u001b[0m         coo_tensor \u001b[39m=\u001b[39m coo_zeros(\n\u001b[0;32m   1637\u001b[0m             A\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], A\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m], nnz_row_ptr[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mitem(), device\n\u001b[0;32m   1638\u001b[0m         )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 开始聊天循环\n",
    "while True:\n",
    "    inputText = input(\"请输入你想说的话: \")   \n",
    "\n",
    "    promptFormatArgs = {\n",
    "        \"character_name\": character_name,\n",
    "        \"user_name\": user_name,\n",
    "        \"character_persona\": character_persona,\n",
    "        \"user_persona\": user_persona,\n",
    "        \"time\": \"不详\",\n",
    "        \"location\": location if location is not None else \"\",\n",
    "        \"environment\": environment if environment is not None else \"\",\n",
    "        \"identity\": identity if identity is not None else \"\",\n",
    "        \"relationship_description\": relationship_description if relationship_description is not None else \"\",\n",
    "        \"nearest_user_chat\": inputText\n",
    "    }    \n",
    "\n",
    "    # 拼接聊天模板，生成当前的聊天内容\n",
    "    currentPrompt = promptTemplate.format(**promptFormatArgs)    \n",
    "\n",
    "    answer = llm(currentPrompt)\n",
    "    print(user_name + \": \" + inputText)\n",
    "    print(character_name + \": \" + answer)\n",
    "\n",
    "    # 将本次完成的聊天内容存入短期数据库\n",
    "    memory.save_context(\n",
    "        {\"input\": inputText},\n",
    "        {\"output\": answer}\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
